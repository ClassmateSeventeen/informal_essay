{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json, time \n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "#from transformers import BertModel, BertConfig, BertTokenizer, AdamW, get_cosine_schedule_with_warmup\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, get_cosine_schedule_with_warmup\n",
    "from transformers import AdamW\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "bert_path = \"/root/yunzhi/retrieval/bert_model\"    # 该文件夹下存放三个文件（'vocab.txt', 'pytorch_model.bin', 'config.json'）\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_path)   # 初始化分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [00:22, 4405.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 30) (100000, 30) (100000, 30) (100000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Union\n",
    "def data_processor(data_path: str)->Union[list, list, list, list]:\n",
    "    \"\"\"\n",
    "    :param data: 原始数据\n",
    "    :return: 返回处理后的数据, 包括input_ids, input_mask, token_type_id, label\n",
    "    \"\"\"\n",
    "    # 输出预处理\n",
    "    input_ids, input_masks, input_types = [], [], []\n",
    "    label = []\n",
    "    maxlen = 30\n",
    "\n",
    "    with open('./news_title_dataset.csv', encoding='utf-8') as f:\n",
    "\n",
    "        for i, line in tqdm(enumerate(f)):\n",
    "            title, y = line.strip().split('\\t')\n",
    "\n",
    "            # encode_plus会输出一个字典，分别为'input_ids', 'token_type_ids', 'attention_mask'对应的编码\n",
    "            # 根据参数会短则补齐，长则切断\n",
    "            encoder_dict = tokenizer.encode_plus(\n",
    "                text=title,\n",
    "                max_length=maxlen,\n",
    "                padding='max_length',\n",
    "                truncation=True\n",
    "            )\n",
    "            input_ids.append(encoder_dict['input_ids'])\n",
    "            input_masks.append(encoder_dict['attention_mask'])\n",
    "            input_types.append(encoder_dict['token_type_ids'])\n",
    "            label.append(int(y))\n",
    "\n",
    "    input_ids, input_types, input_masks, label = np.array(input_ids), np.array(input_types), np.array(input_masks), np.array(label)\n",
    "    print(input_ids.shape, input_types.shape, input_masks.shape, label.shape)\n",
    "    return input_ids, input_types, input_masks, label\n",
    "\n",
    "input_ids, input_types, input_masks, label = data_processor('./news_title_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000,) [0 1 2 3 4 5 6 7 8 9]\n",
      "(80000, 30) (80000,) (10000, 30) (10000,) (10000, 30) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# 切分训练集、验证集、测试集\n",
    "\n",
    "# 随机打乱索引\n",
    "idxes = np.arange(input_ids.shape[0])\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(idexs)\n",
    "print(idxes.shape, idxes[:10])\n",
    "\n",
    "input_ids_train, input_ids_valid, input_ids_test= input_ids[idexs[:80000]], input_ids[idexs[80000:90000]], input_ids[idexs[90000:]]\n",
    "input_types_train, input_types_valid, input_types_test = input_types[idexs[:80000]], input_types[idexs[80000:90000]], input_types[idexs[90000:]]\n",
    "input_masks_train, input_masks_valid, input_masks_test = input_masks[idexs[:80000]], input_masks[idexs[80000:90000]], input_masks[idexs[90000:]]\n",
    "y_train, y_valid, y_test = label[idexs[:80000]], label[idexs[80000:90000]], label[idexs[90000:]]\n",
    "\n",
    "print(input_ids_train.shape, y_train.shape, input_ids_valid.shape, y_valid.shape, \n",
    "      input_ids_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载到pytorch的DataLoder中\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# 训练集\n",
    "trian_dataset = TensorDataset(torch.LongTensor(input_ids_train),\n",
    "                             torch.LongTensor(input_masks_train),  \n",
    "                             torch.LongTensor(input_types_train), \n",
    "                             torch.LongTensor(y_train))\n",
    "train_sampler = RandomSampler(trian_dataset)\n",
    "train_dataloader = DataLoader(trian_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 验证集\n",
    "valid_dataset = TensorDataset(torch.LongTensor(input_ids_valid), \n",
    "                             torch.LongTensor(input_masks_valid), \n",
    "                             torch.LongTensor(input_types_valid), \n",
    "                             torch.LongTensor(y_valid))\n",
    "\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 测试集\n",
    "test_dataset = TensorDataset(torch.LongTensor(input_ids_test), \n",
    "                            torch.LongTensor(input_masks_test), \n",
    "                            torch.LongTensor(input_types_test))\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "\n",
    "class Bert_Model(nn.Module):\n",
    "    def __init__(self,  bert_path, num_classes=10):\n",
    "        super(Bert_Model, self).__init__()\n",
    "        self.config = BertConfig.from_pretrained(bert_path) # 导入模型超参数\n",
    "        self.bert = BertModel.from_pretrained(bert_path) # 导入预训练模型权重 \n",
    "        self.fc = nn.Linear(self.config.hidden_size, num_classes) # 分类器\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        out_pool = outputs[1] # 池化后的输出 [bs, config.hidden_size]\n",
    "        logit = self.fc(out_pool) # 分类器\n",
    "        return logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:102275338, Trainable parameters:102275338\n"
     ]
    }
   ],
   "source": [
    "# 实例化bert模型\n",
    "\n",
    "def get_parameter_number(model):\n",
    "    # 打印模型参数数量，包括总训练的和不可训练的\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return 'Total parameters:{}, Trainable parameters:{}'.format(total_num, trainable_num)\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 5\n",
    "model = Bert_Model(bert_path).to(DEVICE)\n",
    "print(get_parameter_number(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器\n",
    "# 学习率先线性warmup一个epoch，然后cosine式下降。\n",
    "# 这里给个小提示，一定要加warmup（学习率从0慢慢升上去），如果把warmup去掉，可能收敛不了。\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=len(train_dataloader), \n",
    "                                            num_training_steps=len(train_dataloader)*EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练函数和验证测试函数\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    val_true, val_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, att, tpe, y) in (enumerate(data_loader)):\n",
    "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
    "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
    "            val_pred.extend(y_pred)\n",
    "            val_true.extend(y.squeeze().cpu().numpy().tolist())\n",
    "    \n",
    "    return accuracy_score(val_true, val_pred) # 返回accuracy\n",
    "\n",
    "\n",
    "# 测试集没有标签，需要预测提交\n",
    "def predict(model, data_loader, device):\n",
    "    model.eval()\n",
    "    val_pred = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, att, tpe) in (enumerate(data_loader)):\n",
    "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
    "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
    "            val_pred.extend(y_pred)\n",
    "    return val_pred\n",
    "\n",
    "\n",
    "def trian_and_eval(model, train_loader, valid_loader, \n",
    "                  optimizer, scheduler, device, epoch):    \n",
    "    best_acc = 0.0\n",
    "    patience = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for i in range(epoch):\n",
    "        \"\"\"训练模型\"\"\"\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        print(\"***** Running training epoch {} *****\".format(i+1))\n",
    "        train_loss_sum = 0.0\n",
    "        for idx, (ids, att, tpe, y) in (enumerate(train_loader)):\n",
    "            ids, att, tpe, y = ids.to(device), att.to(device), tpe.to(device), y.to(device)\n",
    "            y_pred = model(ids, att, tpe)\n",
    "            loss = criterion(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step() # 学习率变化\n",
    "\n",
    "            train_loss_sum += loss.item()\n",
    "            if (idx + 1) % (len(train_loader)//5) ==0: # 只打印五次结果\n",
    "                print(\"Epoch {:04d} | Step {:04d}/{:04d} | Loss {:.4f} | Time {:.4f}s\".format(\n",
    "                    i + 1, idx + 1, len(train_loader), train_loss_sum/(idx + 1), time.time() - start))\n",
    "                \n",
    "        \"\"\"验证模型\"\"\"\n",
    "        model.eval()\n",
    "        acc = evaluate(model, valid_loader, device) # 验证模型的性能\n",
    "\n",
    "        #保存最优模型\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), 'best_bert.pth')\n",
    "        \n",
    "        print(\"current acc is {:.4f}, best acc is {:.4f}\".format(acc, best_acc))\n",
    "        print(\"time costed = {}s \\n\".format(round(time.time() - start, 5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training epoch 1 *****\n",
      "Epoch 0001 | Step 0125/0625 | Loss 2.4711 | Time 16.5549s\n",
      "Epoch 0001 | Step 0250/0625 | Loss 2.4701 | Time 33.2258s\n",
      "Epoch 0001 | Step 0375/0625 | Loss 2.4698 | Time 50.0695s\n",
      "Epoch 0001 | Step 0500/0625 | Loss 2.4695 | Time 66.9699s\n",
      "Epoch 0001 | Step 0625/0625 | Loss 2.4682 | Time 83.9663s\n",
      "current acc is 0.0772, best acc is 0.0772\n",
      "time costed = 88.36554s \n",
      "\n",
      "***** Running training epoch 2 *****\n",
      "Epoch 0002 | Step 0125/0625 | Loss 2.4673 | Time 17.0088s\n",
      "Epoch 0002 | Step 0250/0625 | Loss 2.4657 | Time 34.0161s\n",
      "Epoch 0002 | Step 0375/0625 | Loss 2.4663 | Time 51.0031s\n",
      "Epoch 0002 | Step 0500/0625 | Loss 2.4680 | Time 68.1117s\n",
      "Epoch 0002 | Step 0625/0625 | Loss 2.4690 | Time 85.1929s\n",
      "current acc is 0.0772, best acc is 0.0772\n",
      "time costed = 89.05188s \n",
      "\n",
      "***** Running training epoch 3 *****\n",
      "Epoch 0003 | Step 0125/0625 | Loss 2.4696 | Time 17.0297s\n",
      "Epoch 0003 | Step 0250/0625 | Loss 2.4666 | Time 34.0879s\n",
      "Epoch 0003 | Step 0375/0625 | Loss 2.4702 | Time 51.0905s\n",
      "Epoch 0003 | Step 0500/0625 | Loss 2.4701 | Time 68.1157s\n",
      "Epoch 0003 | Step 0625/0625 | Loss 2.4699 | Time 85.1525s\n",
      "current acc is 0.0772, best acc is 0.0772\n",
      "time costed = 89.01268s \n",
      "\n",
      "***** Running training epoch 4 *****\n",
      "Epoch 0004 | Step 0125/0625 | Loss 2.4680 | Time 17.0099s\n",
      "Epoch 0004 | Step 0250/0625 | Loss 2.4687 | Time 34.0170s\n",
      "Epoch 0004 | Step 0375/0625 | Loss 2.4696 | Time 51.0289s\n",
      "Epoch 0004 | Step 0500/0625 | Loss 2.4697 | Time 68.0556s\n",
      "Epoch 0004 | Step 0625/0625 | Loss 2.4692 | Time 85.4272s\n",
      "current acc is 0.0772, best acc is 0.0772\n",
      "time costed = 89.28259s \n",
      "\n",
      "***** Running training epoch 5 *****\n",
      "Epoch 0005 | Step 0125/0625 | Loss 2.4656 | Time 17.0485s\n",
      "Epoch 0005 | Step 0250/0625 | Loss 2.4683 | Time 34.0811s\n",
      "Epoch 0005 | Step 0375/0625 | Loss 2.4650 | Time 51.1037s\n",
      "Epoch 0005 | Step 0500/0625 | Loss 2.4671 | Time 68.1107s\n",
      "Epoch 0005 | Step 0625/0625 | Loss 2.4691 | Time 85.1416s\n",
      "current acc is 0.0772, best acc is 0.0772\n",
      "time costed = 89.01169s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 训练和验证模型\n",
    "trian_and_eval(model, train_dataloader, valid_dataloader, optimizer, scheduler, DEVICE, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Accuracy = 0.0816 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000      1013\n",
      "           1     0.1068    0.2268    0.1452      1036\n",
      "           2     0.0631    0.3388    0.1063      1039\n",
      "           3     0.0856    0.0737    0.0792      1031\n",
      "           4     0.1150    0.1582    0.1332       967\n",
      "           5     0.0000    0.0000    0.0000       997\n",
      "           6     0.0000    0.0000    0.0000       985\n",
      "           7     0.0000    0.0000    0.0000       967\n",
      "           8     0.0000    0.0000    0.0000      1031\n",
      "           9     0.0000    0.0000    0.0000       934\n",
      "\n",
      "    accuracy                         0.0816     10000\n",
      "   macro avg     0.0370    0.0798    0.0464     10000\n",
      "weighted avg     0.0376    0.0816    0.0471     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载最优权重对测试集进行测试\n",
    "model.load_state_dict(torch.load('best_bert.pth'))\n",
    "pred_test = predict(model, test_dataloader, DEVICE)\n",
    "print(\"\\n Test Accuracy = {} \\n\".format(accuracy_score(y_test, pred_test)))\n",
    "print(classification_report(y_test, pred_test, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
